{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"home cluster \u00b6 Welcome to the docs on my home Kubernetes cluster. story \u00b6 Two years ago my homelab setup was very different that what it is today. I was running a Docker Swarm cluster and doing deployments with Portainer . While this was working, I was unsure about Docker Swarm's future and I was also looking at a better way to handle application updates. I knew I wanted to keep using open source software and decided it was a good time to start getting familiar with Kubernetes... in came k3s. k3s \u00b6 k3s comes with a very low barrier to entry in getting a Kubernetes cluster running. After deploying it with k3sup I fell in love with the simplicity and love the single binary approach. With k3sup the time it took from getting a Kubernetes cluster up and running was literally minutes. Without much Kubernetes knowledge this allowed me to teardown the cluster and install it again, each time learning new things like how to deploy applications with Helm , set up a load balancer using Metallb , how to handle ingress and storage. flux \u00b6 After awhile of tinkering with k3s, I started reading up on GitOps principles and fell in love with the idea of having a Git repository drive a Kubernetes cluster state. No more missing configuration files, backing up compose files in fear of losing them. I could have mostly everything Kubernetes cares about tracked in a Git repo, while having an operator running in my cluster reading from my Git repo. This is where Flux comes into play. Flux is an awesome tool that syncs my Git repo with my cluster. Any change I make in my Git repo will be directly applied by Flux in my Kubernetes cluster. renovate \u00b6 So I have my cluster running, I have Flux running inside and it is synced to my Git repository. How do I handle application updates? Flux has this built into their application using the image-automation-controller but I am not a fan of having applications auto-update. Sometimes I want to read application changelogs or review source code before I commit to upgrading. I decided that Renovate would be a good solution to my problem.Renovate works by scanning my Git repository and offering changes in the form of a pull request when a new container image update or helm chart update is found. conclusion \u00b6 Overall I am very happy being off my Portainer/Docker Swarm cluster and finally using Kubernetes. It should go without saying a lot of what is built upon here is not only my ideas. A lot of work is being done by other people in the k8s@home community.","title":"Introduction"},{"location":"#home-cluster","text":"Welcome to the docs on my home Kubernetes cluster.","title":"home cluster"},{"location":"#story","text":"Two years ago my homelab setup was very different that what it is today. I was running a Docker Swarm cluster and doing deployments with Portainer . While this was working, I was unsure about Docker Swarm's future and I was also looking at a better way to handle application updates. I knew I wanted to keep using open source software and decided it was a good time to start getting familiar with Kubernetes... in came k3s.","title":"story"},{"location":"#k3s","text":"k3s comes with a very low barrier to entry in getting a Kubernetes cluster running. After deploying it with k3sup I fell in love with the simplicity and love the single binary approach. With k3sup the time it took from getting a Kubernetes cluster up and running was literally minutes. Without much Kubernetes knowledge this allowed me to teardown the cluster and install it again, each time learning new things like how to deploy applications with Helm , set up a load balancer using Metallb , how to handle ingress and storage.","title":"k3s"},{"location":"#flux","text":"After awhile of tinkering with k3s, I started reading up on GitOps principles and fell in love with the idea of having a Git repository drive a Kubernetes cluster state. No more missing configuration files, backing up compose files in fear of losing them. I could have mostly everything Kubernetes cares about tracked in a Git repo, while having an operator running in my cluster reading from my Git repo. This is where Flux comes into play. Flux is an awesome tool that syncs my Git repo with my cluster. Any change I make in my Git repo will be directly applied by Flux in my Kubernetes cluster.","title":"flux"},{"location":"#renovate","text":"So I have my cluster running, I have Flux running inside and it is synced to my Git repository. How do I handle application updates? Flux has this built into their application using the image-automation-controller but I am not a fan of having applications auto-update. Sometimes I want to read application changelogs or review source code before I commit to upgrading. I decided that Renovate would be a good solution to my problem.Renovate works by scanning my Git repository and offering changes in the form of a pull request when a new container image update or helm chart update is found.","title":"renovate"},{"location":"#conclusion","text":"Overall I am very happy being off my Portainer/Docker Swarm cluster and finally using Kubernetes. It should go without saying a lot of what is built upon here is not only my ideas. A lot of work is being done by other people in the k8s@home community.","title":"conclusion"},{"location":"external-secrets/","text":"external-secrets \u00b6 Work in progress This document is a work in progress. Create secret for External Secrets using AWS Secrets Manager \u00b6 kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system Create a secret using aws-cli \u00b6 aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"external-secrets"},{"location":"external-secrets/#external-secrets","text":"Work in progress This document is a work in progress.","title":"external-secrets"},{"location":"external-secrets/#create-secret-for-external-secrets-using-aws-secrets-manager","text":"kubectl create secret generic aws-credentials \\ --from-literal = id = \"access-key-id\" \\ --from-literal = key = \"access-secret-key\" \\ --namespace kube-system","title":"Create secret for External Secrets using AWS Secrets Manager"},{"location":"external-secrets/#create-a-secret-using-aws-cli","text":"aws secretsmanager create-secret \\ --name namespace/secret-name \\ --secret-string \"secret-data\"","title":"Create a secret using aws-cli"},{"location":"flux/","text":"flux \u00b6 Work in progress This document is a work in progress. install cli tool \u00b6 brew install fluxcd/tap/flux install cluster components \u00b6 For full installation guide visit the Flux installation guide Set the GITHUB_TOKEN environment variable to a personal access token you created in Github. export GITHUB_TOKEN = 47241b5a1f9cc45cc7388cf787fc6abacf99eb70 Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster flux bootstrap github \\ --version = v0.9.0 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster \\ --personal \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work useful commands \u00b6 # force flux to sync your repository flux reconcile source git flux-system # force flux to sync a helm release flux reconcile helmrelease sonarr -n default # force flux to sync a helm repository flux reconcile source helm ingress-nginx-charts -n flux-system","title":"Flux"},{"location":"flux/#flux","text":"Work in progress This document is a work in progress.","title":"flux"},{"location":"flux/#install-cli-tool","text":"brew install fluxcd/tap/flux","title":"install cli tool"},{"location":"flux/#install-cluster-components","text":"For full installation guide visit the Flux installation guide Set the GITHUB_TOKEN environment variable to a personal access token you created in Github. export GITHUB_TOKEN = 47241b5a1f9cc45cc7388cf787fc6abacf99eb70 Check if you cluster is ready for Flux flux check --pre Install Flux into your cluster flux bootstrap github \\ --version = v0.9.0 \\ --owner = onedr0p \\ --repository = home-cluster \\ --path = cluster \\ --personal \\ --network-policy = false Note : When using k3s I found that the network-policy flag has to be set to false, or Flux will not work","title":"install cluster components"},{"location":"flux/#useful-commands","text":"# force flux to sync your repository flux reconcile source git flux-system # force flux to sync a helm release flux reconcile helmrelease sonarr -n default # force flux to sync a helm repository flux reconcile source helm ingress-nginx-charts -n flux-system","title":"useful commands"},{"location":"pxe/","text":"ubuntu 20.04 Network boot using opnsense \u00b6 opnsense settings \u00b6 Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"ubuntu 20.04 Network boot using opnsense"},{"location":"pxe/#ubuntu-2004-network-boot-using-opnsense","text":"","title":"ubuntu 20.04 Network boot using opnsense"},{"location":"pxe/#opnsense-settings","text":"Enable dnsmasq in the Opnsense services settings (set port to 63 ) Copy over pxe.conf to /usr/local/etc/dnsmasq.conf.d/pxe.conf SSH into opnsense and run the following commands... $ mkdir -p /var/lib/tftpboot/pxelinux/ $ wget https://releases.ubuntu.com/20.04/ubuntu-20.04.2-live-server-amd64.iso -O /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso $ mount -t cd9660 /dev/ ` mdconfig -f /var/lib/tftpboot/ubuntu-20.04.2-live-server-amd64.iso ` /mnt $ cp /mnt/casper/vmlinuz /var/lib/tftpboot/pxelinux/ $ cp /mnt/casper/initrd /var/lib/tftpboot/pxelinux/ $ umount /mnt $ wget http://archive.ubuntu.com/ubuntu/dists/focal/main/uefi/grub2-amd64/current/grubnetx64.efi.signed -O /var/lib/tftpboot/pxelinux/pxelinux.0 Copy grub/grub.conf into /var/lib/tftpboot/grub/grub.conf Copy nodes/ into /var/lib/tftpboot/nodes","title":"opnsense settings"},{"location":"rook-ceph/","text":"rook-ceph \u00b6 https://rook.io/docs/rook/v1.2/ceph-common-issues.html Toolbox \u00b6 kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash Crashes \u00b6 Sometime ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthly do the following... ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all Migration \u00b6 In your shell... # Scale app to 0 replicas kubectl scale deploy/zigbee2mqtt --replicas 0 -n home # Get RBD image name for the app kubectl get pv/ ( k get pv | grep zigbee2mqtt-data | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' # csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 In another shell tab... kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash # create mount directories mkdir -p /mnt/ { tmp,Data } # mount nfs with backups mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=1048576,wsize=1048576,hard\" 192 .168.1.40:/volume1/Data /mnt/Data # optional list rbds rbd list --pool replicapool rbd map -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 mount /dev/rbd0 /mnt/tmp tar xvf /mnt/Data/backups/zigbee2mqtt.tar.gz -C /mnt/tmp chown -R 568 :568 /mnt/tmp/ umount /mnt/tmp rbd unmap -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25","title":"rook-ceph"},{"location":"rook-ceph/#rook-ceph","text":"https://rook.io/docs/rook/v1.2/ceph-common-issues.html","title":"rook-ceph"},{"location":"rook-ceph/#toolbox","text":"kubectl -n rook-ceph exec -it $( kubectl -n rook-ceph get pod -l \"app=rook-ceph-tools\" -o jsonpath = '{.items[0].metadata.name}' ) bash","title":"Toolbox"},{"location":"rook-ceph/#crashes","text":"Sometime ceph will report a HEALTH_WARN even when the health is fine, in order to get ceph to report back healthly do the following... ceph crash ls # if you want to read the message ceph crash info <id> # archive crash report ceph crash archive <id> # or, archive all crash reports ceph crash archive-all","title":"Crashes"},{"location":"rook-ceph/#migration","text":"In your shell... # Scale app to 0 replicas kubectl scale deploy/zigbee2mqtt --replicas 0 -n home # Get RBD image name for the app kubectl get pv/ ( k get pv | grep zigbee2mqtt-data | awk -F ' ' '{print $1}' ) -n home -o json | jq -r '.spec.csi.volumeAttributes.imageName' # csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 In another shell tab... kubectl -n rook-ceph exec -it ( kubectl -n rook-ceph get pod -l \"app=rook-direct-mount\" -o jsonpath = '{.items[0].metadata.name}' ) bash # create mount directories mkdir -p /mnt/ { tmp,Data } # mount nfs with backups mount -t nfs -o \"tcp,intr,rw,noatime,nodiratime,rsize=1048576,wsize=1048576,hard\" 192 .168.1.40:/volume1/Data /mnt/Data # optional list rbds rbd list --pool replicapool rbd map -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25 mount /dev/rbd0 /mnt/tmp tar xvf /mnt/Data/backups/zigbee2mqtt.tar.gz -C /mnt/tmp chown -R 568 :568 /mnt/tmp/ umount /mnt/tmp rbd unmap -p replicapool csi-vol-e4a2e40f-2795-11eb-80c7-2298c6796a25","title":"Migration"},{"location":"sealed-secrets/","text":"sealed-secrets \u00b6 Work in progress This document is a work in progress. install cli tool \u00b6 brew install kubeseal install cluster components \u00b6 --- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : sealed-secrets namespace : kube-system spec : interval : 5m chart : spec : chart : sealed-secrets version : 1.13.2 sourceRef : kind : HelmRepository name : sealed-secrets-charts namespace : flux-system interval : 5m values : ingress : enabled : false fetch sealed secrets cert \u00b6 kubeseal \\ --controller-name sealed-secrets \\ --fetch-cert > ./sealed-secrets-public-cert.pem","title":"Sealed Secrets"},{"location":"sealed-secrets/#sealed-secrets","text":"Work in progress This document is a work in progress.","title":"sealed-secrets"},{"location":"sealed-secrets/#install-cli-tool","text":"brew install kubeseal","title":"install cli tool"},{"location":"sealed-secrets/#install-cluster-components","text":"--- apiVersion : helm.toolkit.fluxcd.io/v2beta1 kind : HelmRelease metadata : name : sealed-secrets namespace : kube-system spec : interval : 5m chart : spec : chart : sealed-secrets version : 1.13.2 sourceRef : kind : HelmRepository name : sealed-secrets-charts namespace : flux-system interval : 5m values : ingress : enabled : false","title":"install cluster components"},{"location":"sealed-secrets/#fetch-sealed-secrets-cert","text":"kubeseal \\ --controller-name sealed-secrets \\ --fetch-cert > ./sealed-secrets-public-cert.pem","title":"fetch sealed secrets cert"},{"location":"snmp-exporter/","text":"snmp-exporter \u00b6 Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus clone and build the snmp-exporter generator \u00b6 sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs update generator.yml \u00b6 Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%) get the cyberpower MIB \u00b6 wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/ generate the snmp.yml \u00b6 This will create a snmp.yml file which will be needed for the configmap for snmp-exporter export MIBDIRS = mibs ./generator generate","title":"Snmp Exporter"},{"location":"snmp-exporter/#snmp-exporter","text":"Work in progress This document is a work in progress. I am using snmp-exporter for getting metrics from my Cyberpower PDUs ( PDU41001 ) and my APC UPS ( Smart-UPS 1500 ) into Prometheus","title":"snmp-exporter"},{"location":"snmp-exporter/#clone-and-build-the-snmp-exporter-generator","text":"sudo apt-get install unzip build-essential libsnmp-dev golang go get github.com/prometheus/snmp_exporter/generator cd ${ GOPATH - $HOME /go } /src/github.com/prometheus/snmp_exporter/generator go build make mibs","title":"clone and build the snmp-exporter generator"},{"location":"snmp-exporter/#update-generatoryml","text":"Kubernetes configmap 's have a max size. I needed to strip out all the other modules. modules : apcups : version : 1 walk : - sysUpTime - interfaces - 1.3.6.1.4.1.318.1.1.1.2 # upsBattery - 1.3.6.1.4.1.318.1.1.1.3 # upsInput - 1.3.6.1.4.1.318.1.1.1.4 # upsOutput - 1.3.6.1.4.1.318.1.1.1.7.2 # upsAdvTest - 1.3.6.1.4.1.318.1.1.1.8.1 # upsCommStatus - 1.3.6.1.4.1.318.1.1.1.12 # upsOutletGroups - 1.3.6.1.4.1.318.1.1.10.2.3.2 # iemStatusProbesTable - 1.3.6.1.4.1.318.1.1.26.8.3 # rPDU2BankStatusTable lookups : - source_indexes : [ upsOutletGroupStatusIndex ] lookup : upsOutletGroupStatusName drop_source_indexes : true - source_indexes : [ iemStatusProbeIndex ] lookup : iemStatusProbeName drop_source_indexes : true overrides : ifType : type : EnumAsInfo rPDU2BankStatusLoadState : type : EnumAsStateSet upsAdvBatteryCondition : type : EnumAsStateSet upsAdvBatteryChargingCurrentRestricted : type : EnumAsStateSet upsAdvBatteryChargerStatus : type : EnumAsStateSet cyberpower : version : 1 walk : - ePDUIdentName - ePDUIdentHardwareRev - ePDUStatusInputVoltage ## input voltage (0.1 volts) - ePDUStatusInputFrequency ## input frequency (0.1 Hertz) - ePDULoadStatusLoad ## load (tenths of Amps) - ePDULoadStatusVoltage ## voltage (0.1 volts) - ePDULoadStatusActivePower ## active power (watts) - ePDULoadStatusApparentPower ## apparent power (VA) - ePDULoadStatusPowerFactor ## power factor of the output (hundredths) - ePDULoadStatusEnergy ## apparent power measured (0.1 kw/h). - ePDUOutletControlOutletName ## The name of the outlet. - ePDUOutletStatusLoad ## Outlet load (tenths of Amps) - ePDUOutletStatusActivePower ## Outlet load (watts) - envirTemperature ## temp expressed (1/10 \u00baF) - envirTemperatureCelsius ## temp expressed (1/10 \u00baF) - envirHumidity ## relative humidity (%)","title":"update generator.yml"},{"location":"snmp-exporter/#get-the-cyberpower-mib","text":"wget https://dl4jz3rbrsfum.cloudfront.net/software/CyberPower_MIB_v2.9.MIB.zip unzip CyberPower_MIB_v2.9.MIB.zip mv CyberPower_MIB_v2.9.MIB mibs/","title":"get the cyberpower MIB"},{"location":"snmp-exporter/#generate-the-snmpyml","text":"This will create a snmp.yml file which will be needed for the configmap for snmp-exporter export MIBDIRS = mibs ./generator generate","title":"generate the snmp.yml"},{"location":"velero/","text":"velero \u00b6 Velero is a cluster backup & restore solution. I can also leverage restic to backup persistent volumes to S3 storage buckets. In order to backup and restore a given workload, the following steps should work. install cli tool \u00b6 brew install velero backup \u00b6 A backup should already be created by either a scheduled, or manual backup # create a backup for all apps velero backup create manually-backup-1 --from-schedule velero-daily-backup # create a backup for a single app velero backup create jackett-test-abc --include-namespaces testing --selector \"app.kubernetes.io/instance=jackett-test\" --wait delete resources \u00b6 # delete the helmrelease kubectl delete hr jackett-test -n testing # allow the application to redeployed and create the new resources # delete the new resources kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config restore \u00b6 velero restore create --from-backup velero-daily-backup-20201120020022 --include-namespaces testing --selector \"app.kubernetes.io/instance=jackett-test\" --wait This should not interfere with the HelmRelease or require scaling helm-operator You don't need to worry about adding labels to the HelmRelease or backing-up the helm secret object","title":"velero"},{"location":"velero/#velero","text":"Velero is a cluster backup & restore solution. I can also leverage restic to backup persistent volumes to S3 storage buckets. In order to backup and restore a given workload, the following steps should work.","title":"velero"},{"location":"velero/#install-cli-tool","text":"brew install velero","title":"install cli tool"},{"location":"velero/#backup","text":"A backup should already be created by either a scheduled, or manual backup # create a backup for all apps velero backup create manually-backup-1 --from-schedule velero-daily-backup # create a backup for a single app velero backup create jackett-test-abc --include-namespaces testing --selector \"app.kubernetes.io/instance=jackett-test\" --wait","title":"backup"},{"location":"velero/#delete-resources","text":"# delete the helmrelease kubectl delete hr jackett-test -n testing # allow the application to redeployed and create the new resources # delete the new resources kubectl delete deployment/jackett-test -n jackett kubectl delete pvc/jackett-test-config","title":"delete resources"},{"location":"velero/#restore","text":"velero restore create --from-backup velero-daily-backup-20201120020022 --include-namespaces testing --selector \"app.kubernetes.io/instance=jackett-test\" --wait This should not interfere with the HelmRelease or require scaling helm-operator You don't need to worry about adding labels to the HelmRelease or backing-up the helm secret object","title":"restore"}]}